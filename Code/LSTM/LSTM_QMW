{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN+KL4zXWaz5mNrkahoB47p"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"mKIWfaQYbcGh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715459690758,"user_tz":240,"elapsed":5168,"user":{"displayName":"Rafiz Sadique","userId":"04609178330490164874"}},"outputId":"62a3fb5b-2fd3-4585-8f30-166ff6828711"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Keys of loaded dataframes:\n","dict_keys(['train_data_w', 'test_data_w', 'train_data_m', 'test_data_m', 'train_data_q', 'test_data_q'])\n","Sizes of loaded dataframes:\n","train_data_w: (100000, 3)\n","test_data_w: (20000, 3)\n","train_data_m: (100000, 3)\n","test_data_m: (20000, 3)\n","train_data_q: (100000, 3)\n","test_data_q: (20000, 3)\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import zipfile\n","import os\n","from datetime import datetime\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","base_path = '/content/drive/My Drive/519_Project_V1/content/'\n","\n","# Define the paths to the ZIP files\n","zip_path = base_path + 'all_tesla_data.zip'\n","\n","# Unzip and load dataframes\n","dataframes = {}\n","\n","with zipfile.ZipFile(zip_path, 'r') as zipf:\n","    for filename in zipf.namelist():\n","        if filename.endswith('.csv'):\n","            with zipf.open(filename) as file:\n","                df_name = filename.replace('.csv', '').replace('-', '_')\n","                dataframes[df_name] = pd.read_csv(file)\n","\n","# Printing the keys of the loaded dataframes\n","print(\"Keys of loaded dataframes:\")\n","print(dataframes.keys())\n","\n","# Printing the sizes of the loaded dataframes\n","print(\"Sizes of loaded dataframes:\")\n","for name, df in dataframes.items():\n","    print(f\"{name}: {df.shape}\")\n"]},{"cell_type":"code","source":["import re\n","\n","# Function to clean tweets\n","def clean_tweets(tweet):\n","    tweet = re.sub(\"@[A-Za-z0-9]+\", \"\", tweet)  # Remove @mentions\n","    tweet = re.sub(r\"https?\\:\\/\\/[\\w\\.\\/]+\", \"\", tweet)  # Remove URLs\n","    tweet = re.sub(r\"[^A-Za-z0-9 ]+\", \"\", tweet)  # Remove all non-alphanumeric characters except spaces\n","    tweet = tweet.lower()  # Convert to lowercase to avoid duplicates from capitalization\n","    tweet = \" \".join(tweet.split())  # Remove redundant spaces\n","    return tweet\n","\n","# Apply the cleaning function to each tweet in the DataFrames\n","for name, df in dataframes.items():\n","    df['cleaned_body'] = df['body'].apply(clean_tweets)\n","\n","# Printing the new sizes of the DataFrames after cleaning\n","print(\"New sizes of dataframes after cleaning:\")\n","for name, df in dataframes.items():\n","    print(f\"{name}: {df.shape}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iByUtUlnANkr","executionInfo":{"status":"ok","timestamp":1715459695815,"user_tz":240,"elapsed":5071,"user":{"displayName":"Rafiz Sadique","userId":"04609178330490164874"}},"outputId":"70609d00-4612-4410-9cdb-dd95234194c5"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["New sizes of dataframes after cleaning:\n","train_data_w: (100000, 4)\n","test_data_w: (20000, 4)\n","train_data_m: (100000, 4)\n","test_data_m: (20000, 4)\n","train_data_q: (100000, 4)\n","test_data_q: (20000, 4)\n"]}]},{"cell_type":"code","source":["# Remove duplicates based on the 'cleaned_body' column\n","for name, df in dataframes.items():\n","    df = df.drop_duplicates(subset=['cleaned_body'], inplace=True)\n","\n","# Printing the new sizes of the DataFrames after removing duplicates\n","print(\"New sizes of dataframes after removing duplicates:\")\n","for name, df in dataframes.items():\n","    print(f\"{name}: {df.shape}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ranKnZuHAQKS","executionInfo":{"status":"ok","timestamp":1715459696223,"user_tz":240,"elapsed":416,"user":{"displayName":"Rafiz Sadique","userId":"04609178330490164874"}},"outputId":"0e1bb017-696a-4c64-d7cf-ff7671f97d0f"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["New sizes of dataframes after removing duplicates:\n","train_data_w: (97314, 4)\n","test_data_w: (19716, 4)\n","train_data_m: (97314, 4)\n","test_data_m: (19716, 4)\n","train_data_q: (97314, 4)\n","test_data_q: (19716, 4)\n"]}]},{"cell_type":"code","source":["# Randomly select 5000 tweets from each DataFrame\n","num_samples = 7500\n","\n","for name, df in dataframes.items():\n","    if df.shape[0] > num_samples:\n","        dataframes[name] = df.sample(n=num_samples, random_state=42)\n","\n","# Printing the new sizes of the DataFrames after sampling\n","print(\"New sizes of dataframes after sampling:\")\n","for name, df in dataframes.items():\n","    print(f\"{name}: {df.shape}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BLr_xd1eCGFA","executionInfo":{"status":"ok","timestamp":1715459696457,"user_tz":240,"elapsed":237,"user":{"displayName":"Rafiz Sadique","userId":"04609178330490164874"}},"outputId":"1d38d911-c527-4886-8d9d-e64d9aec51b2"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["New sizes of dataframes after sampling:\n","train_data_w: (7500, 4)\n","test_data_w: (7500, 4)\n","train_data_m: (7500, 4)\n","test_data_m: (7500, 4)\n","train_data_q: (7500, 4)\n","test_data_q: (7500, 4)\n"]}]},{"cell_type":"code","source":["# Import Word2Vec from gensim\n","from gensim.models import Word2Vec\n","\n","# Prepare the list of tokenized tweets for Word2Vec training\n","# Concatenate all tweets from different datasets into a single list for training\n","all_tweets = []\n","for name, df in dataframes.items():\n","    all_tweets.extend(df['cleaned_body'].apply(lambda x: x.split()).tolist())\n","\n","# Train a Word2Vec model\n","word2vec_model = Word2Vec(sentences=all_tweets, vector_size=100, window=5, min_count=1, workers=4)\n","\n","# Save the model for later use\n","word2vec_model.save(\"/content/drive/My Drive/519_Project_V1/LSTM/word2vec_model.bin\")\n","\n","# Check the word vector for a sample word to ensure training\n","print(\"Vector for the word 'tsla':\")\n","print(word2vec_model.wv['tsla'])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M14W8DzRENfm","executionInfo":{"status":"ok","timestamp":1715459711095,"user_tz":240,"elapsed":14639,"user":{"displayName":"Rafiz Sadique","userId":"04609178330490164874"}},"outputId":"fe2a0c8d-204b-40c1-ba8d-0318c7bf0235"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Vector for the word 'tsla':\n","[ 0.14427757 -0.3621457   0.6468504  -0.15650521 -0.8918414  -0.9825105\n","  0.7430051  -0.20084551  0.29484418  0.850946    0.46710306  0.87116796\n","  0.24321029 -0.14250611 -0.85539705 -0.01985401  0.92629886  0.2550928\n","  0.89289415 -0.5386692  -0.34468797  0.57786     0.49376062  0.89553356\n","  0.82039636 -1.1063838  -0.48449287  2.2415004   0.4378977  -1.3620608\n"," -0.78317463  0.16390128  0.6067619  -1.3415818  -1.767442    0.728001\n","  1.5871961  -1.2365977  -0.80935717 -0.41026556 -1.1296786   0.33926648\n"," -0.70195305 -0.5601479   0.70824295 -0.27391824 -0.87038887  1.2471404\n","  0.57000554  0.7491604  -0.31506565  0.12561972 -0.6225283   0.4120063\n","  0.32816917 -0.6458812   0.00764369  0.02606739 -0.3765324   0.20987758\n"," -0.5115669  -0.63687915  0.82287115 -0.27923283 -0.6052928   1.4793699\n","  0.81275624 -0.25732857 -0.35264635  0.9274659  -0.38762096  1.010922\n"," -0.5608992  -1.4817219   1.3461477   0.76330763 -0.6872516   0.25197947\n"," -0.34735188  0.222215   -0.43799362  0.53098786 -0.2892781   1.6614485\n","  0.73939484  1.6489526   1.3814235  -0.02242511 -0.40481186  1.1547914\n","  0.19701217  0.67820615  0.7710589  -0.68784577  1.4969875   0.72964424\n"," -0.15308887 -0.9662017   0.3895155  -0.8327384 ]\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","\n","# Function to vectorize a batch of tweets\n","def vectorize_tweets(tweets, word2vec_model, max_length):\n","    vectorized_tweets = []\n","    for tweet in tweets:\n","        tweet_vector = [word2vec_model.wv[word] for word in tweet if word in word2vec_model.wv]\n","        vectorized_tweets.append(tweet_vector)\n","\n","    # Pad the sequences to have the same length\n","    #from tensorflow.keras.preprocessing.sequence import pad_sequences\n","    vectorized_tweets = pad_sequences(vectorized_tweets, maxlen=max_length, dtype='float32', padding='post', truncating='post', value=0.0)\n","    return vectorized_tweets\n","\n","\n","# Define the maximum length of a tweet vector sequence\n","max_length = 50  # Adjust based on the average length of your tweets\n","\n","# Vectorize the tweets in each DataFrame and store them\n","vectorized_dataframes = {}\n","for name, df in dataframes.items():\n","    tokenized_tweets = df['cleaned_body'].apply(lambda x: x.split()).tolist()\n","    vectorized_dataframes[name] = vectorize_tweets(tokenized_tweets, word2vec_model, max_length)\n","\n","# Print shapes of the vectorized data\n","for name, vectors in vectorized_dataframes.items():\n","    print(f\"Vectorized data for {name}: {vectors.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uOeJABjOEPG7","executionInfo":{"status":"ok","timestamp":1715459717778,"user_tz":240,"elapsed":6701,"user":{"displayName":"Rafiz Sadique","userId":"04609178330490164874"}},"outputId":"36a2a7b5-9517-4978-fcf1-45b326553170"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Vectorized data for train_data_w: (7500, 50, 100)\n","Vectorized data for test_data_w: (7500, 50, 100)\n","Vectorized data for train_data_m: (7500, 50, 100)\n","Vectorized data for test_data_m: (7500, 50, 100)\n","Vectorized data for train_data_q: (7500, 50, 100)\n","Vectorized data for test_data_q: (7500, 50, 100)\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding\n","\n","# Define the LSTM model function\n","def create_lstm_model(input_shape):\n","    model = Sequential()\n","\n","    # Add an LSTM layer with 50 units\n","    model.add(LSTM(50, return_sequences=True, input_shape=input_shape))\n","    model.add(Dropout(0.2))\n","\n","    # Add another LSTM layer with 50 units, without return_sequences to flatten the output\n","    model.add(LSTM(50))\n","    model.add(Dropout(0.2))\n","\n","    # Add a Dense layer for the output\n","    model.add(Dense(25, activation='relu'))\n","    model.add(Dense(1, activation='sigmoid'))  # Use sigmoid for binary classification\n","\n","    # Compile the model\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","    return model\n","\n","# Use the shape of the vectorized data to define the input shape for the LSTM model\n","input_shape = (vectorized_dataframes['train_data_w'].shape[1], vectorized_dataframes['train_data_w'].shape[2])\n","\n","# Create the LSTM model using the defined function\n","lstm_model = create_lstm_model(input_shape)\n","\n","# Print the model summary to review the architecture\n","lstm_model.summary()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IRftMVD7FSOb","executionInfo":{"status":"ok","timestamp":1715459719037,"user_tz":240,"elapsed":1275,"user":{"displayName":"Rafiz Sadique","userId":"04609178330490164874"}},"outputId":"1f3cb30b-f67f-4db8-8bf6-838b81b9af5d"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," lstm (LSTM)                 (None, 50, 50)            30200     \n","                                                                 \n"," dropout (Dropout)           (None, 50, 50)            0         \n","                                                                 \n"," lstm_1 (LSTM)               (None, 50)                20200     \n","                                                                 \n"," dropout_1 (Dropout)         (None, 50)                0         \n","                                                                 \n"," dense (Dense)               (None, 25)                1275      \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 26        \n","                                                                 \n","=================================================================\n","Total params: 51701 (201.96 KB)\n","Trainable params: 51701 (201.96 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","# Function to evaluate the model\n","def evaluate_model(model, X_test, y_test):\n","    # Predict the labels for the test set\n","    y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n","\n","    # Calculate evaluation metrics\n","    accuracy = accuracy_score(y_test, y_pred)\n","    precision = precision_score(y_test, y_pred)\n","    recall = recall_score(y_test, y_pred)\n","    f1 = f1_score(y_test, y_pred)\n","\n","    return accuracy, precision, recall, f1\n","\n","# Dictionary to store performance metrics\n","performance_metrics = {}\n","\n","# Train and evaluate for each dataset\n","for key in ['train_data_w', 'train_data_m', 'train_data_q']:\n","    # Extract the train and test sets\n","    train_key = key\n","    test_key = key.replace('train', 'test')\n","\n","    X_train = vectorized_dataframes[train_key]\n","    y_train = dataframes[train_key]['Target'].values\n","    X_test = vectorized_dataframes[test_key]\n","    y_test = dataframes[test_key]['Target'].values\n","\n","    # Create and compile the LSTM model\n","    lstm_model = create_lstm_model((X_train.shape[1], X_train.shape[2]))\n","\n","    # Train the model\n","    lstm_model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2)\n","\n","    # Evaluate the model\n","    accuracy, precision, recall, f1 = evaluate_model(lstm_model, X_test, y_test)\n","\n","    # Store the metrics\n","    performance_metrics[test_key] = {\n","        'Accuracy': accuracy,\n","        'Precision': precision,\n","        'Recall': recall,\n","        'F1 Score': f1\n","    }\n","\n","# Print the performance metrics for each dataset\n","for key, metrics in performance_metrics.items():\n","    print(f\"Performance for {key}:\")\n","    for metric, value in metrics.items():\n","        print(f\"{metric}: {value:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HMba8wWfFvU1","executionInfo":{"status":"ok","timestamp":1715459789409,"user_tz":240,"elapsed":70376,"user":{"displayName":"Rafiz Sadique","userId":"04609178330490164874"}},"outputId":"247a6560-b473-4c88-99ee-b6a2569f33f1"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","94/94 [==============================] - 8s 22ms/step - loss: 0.6868 - accuracy: 0.5618 - val_loss: 0.6867 - val_accuracy: 0.5500\n","Epoch 2/10\n","94/94 [==============================] - 1s 10ms/step - loss: 0.6854 - accuracy: 0.5633 - val_loss: 0.6873 - val_accuracy: 0.5560\n","Epoch 3/10\n","94/94 [==============================] - 1s 14ms/step - loss: 0.6839 - accuracy: 0.5667 - val_loss: 0.6877 - val_accuracy: 0.5553\n","Epoch 4/10\n","94/94 [==============================] - 2s 17ms/step - loss: 0.6838 - accuracy: 0.5697 - val_loss: 0.6871 - val_accuracy: 0.5567\n","Epoch 5/10\n","94/94 [==============================] - 2s 17ms/step - loss: 0.6823 - accuracy: 0.5702 - val_loss: 0.6857 - val_accuracy: 0.5587\n","Epoch 6/10\n","94/94 [==============================] - 1s 11ms/step - loss: 0.6817 - accuracy: 0.5737 - val_loss: 0.6858 - val_accuracy: 0.5607\n","Epoch 7/10\n","94/94 [==============================] - 1s 11ms/step - loss: 0.6798 - accuracy: 0.5708 - val_loss: 0.6856 - val_accuracy: 0.5567\n","Epoch 8/10\n","94/94 [==============================] - 1s 10ms/step - loss: 0.6790 - accuracy: 0.5762 - val_loss: 0.6855 - val_accuracy: 0.5533\n","Epoch 9/10\n","94/94 [==============================] - 1s 10ms/step - loss: 0.6782 - accuracy: 0.5760 - val_loss: 0.6878 - val_accuracy: 0.5580\n","Epoch 10/10\n","94/94 [==============================] - 1s 10ms/step - loss: 0.6728 - accuracy: 0.5802 - val_loss: 0.6950 - val_accuracy: 0.5253\n","235/235 [==============================] - 2s 4ms/step\n","Epoch 1/10\n","94/94 [==============================] - 7s 26ms/step - loss: 0.6903 - accuracy: 0.5317 - val_loss: 0.6927 - val_accuracy: 0.5240\n","Epoch 2/10\n","94/94 [==============================] - 1s 11ms/step - loss: 0.6894 - accuracy: 0.5367 - val_loss: 0.6951 - val_accuracy: 0.5240\n","Epoch 3/10\n","94/94 [==============================] - 1s 11ms/step - loss: 0.6894 - accuracy: 0.5368 - val_loss: 0.6946 - val_accuracy: 0.5240\n","Epoch 4/10\n","94/94 [==============================] - 1s 14ms/step - loss: 0.6884 - accuracy: 0.5343 - val_loss: 0.6919 - val_accuracy: 0.5307\n","Epoch 5/10\n","94/94 [==============================] - 1s 14ms/step - loss: 0.6871 - accuracy: 0.5388 - val_loss: 0.6949 - val_accuracy: 0.5307\n","Epoch 6/10\n","94/94 [==============================] - 2s 17ms/step - loss: 0.6866 - accuracy: 0.5428 - val_loss: 0.6941 - val_accuracy: 0.5293\n","Epoch 7/10\n","94/94 [==============================] - 1s 11ms/step - loss: 0.6841 - accuracy: 0.5442 - val_loss: 0.7002 - val_accuracy: 0.5140\n","Epoch 8/10\n","94/94 [==============================] - 1s 11ms/step - loss: 0.6812 - accuracy: 0.5498 - val_loss: 0.7086 - val_accuracy: 0.5213\n","Epoch 9/10\n","94/94 [==============================] - 1s 12ms/step - loss: 0.6777 - accuracy: 0.5607 - val_loss: 0.7005 - val_accuracy: 0.5280\n","Epoch 10/10\n","94/94 [==============================] - 1s 15ms/step - loss: 0.6744 - accuracy: 0.5610 - val_loss: 0.7083 - val_accuracy: 0.5067\n","235/235 [==============================] - 2s 5ms/step\n","Epoch 1/10\n","94/94 [==============================] - 6s 19ms/step - loss: 0.6751 - accuracy: 0.5978 - val_loss: 0.6783 - val_accuracy: 0.5740\n","Epoch 2/10\n","94/94 [==============================] - 1s 10ms/step - loss: 0.6698 - accuracy: 0.5970 - val_loss: 0.6781 - val_accuracy: 0.5720\n","Epoch 3/10\n","94/94 [==============================] - 1s 11ms/step - loss: 0.6656 - accuracy: 0.6010 - val_loss: 0.6846 - val_accuracy: 0.5760\n","Epoch 4/10\n","94/94 [==============================] - 1s 10ms/step - loss: 0.6651 - accuracy: 0.6003 - val_loss: 0.6817 - val_accuracy: 0.5753\n","Epoch 5/10\n","94/94 [==============================] - 1s 12ms/step - loss: 0.6592 - accuracy: 0.6047 - val_loss: 0.6777 - val_accuracy: 0.5740\n","Epoch 6/10\n","94/94 [==============================] - 1s 14ms/step - loss: 0.6567 - accuracy: 0.5997 - val_loss: 0.6842 - val_accuracy: 0.5727\n","Epoch 7/10\n","94/94 [==============================] - 1s 15ms/step - loss: 0.6536 - accuracy: 0.6027 - val_loss: 0.6815 - val_accuracy: 0.5800\n","Epoch 8/10\n","94/94 [==============================] - 1s 13ms/step - loss: 0.6456 - accuracy: 0.6120 - val_loss: 0.6965 - val_accuracy: 0.5660\n","Epoch 9/10\n","94/94 [==============================] - 1s 11ms/step - loss: 0.6418 - accuracy: 0.6152 - val_loss: 0.7112 - val_accuracy: 0.5680\n","Epoch 10/10\n","94/94 [==============================] - 1s 11ms/step - loss: 0.6418 - accuracy: 0.6140 - val_loss: 0.6820 - val_accuracy: 0.5720\n","235/235 [==============================] - 2s 5ms/step\n","Performance for test_data_w:\n","Accuracy: 0.5012\n","Precision: 0.5006\n","Recall: 0.7011\n","F1 Score: 0.5841\n","Performance for test_data_m:\n","Accuracy: 0.5613\n","Precision: 0.6118\n","Recall: 0.7992\n","F1 Score: 0.6930\n","Performance for test_data_q:\n","Accuracy: 0.3869\n","Precision: 0.7267\n","Recall: 0.2585\n","F1 Score: 0.3813\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","# Function to create an optimized LSTM model\n","def create_optimized_lstm_model(input_shape, lstm_units=50, dropout_rate=0.3, dense_units=25, learning_rate=0.001):\n","    model = Sequential()\n","\n","    # LSTM layer with variable units\n","    model.add(LSTM(lstm_units, return_sequences=True, input_shape=input_shape))\n","    model.add(Dropout(dropout_rate))\n","\n","    # Another LSTM layer without return_sequences\n","    model.add(LSTM(lstm_units))\n","    model.add(Dropout(dropout_rate))\n","\n","    # Dense layer with variable units\n","    model.add(Dense(dense_units, activation='relu'))\n","    model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n","\n","    # Compile the model with a specified learning rate\n","    optimizer = Adam(learning_rate=learning_rate)\n","    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n","\n","    return model\n","\n","# Train and evaluate for each dataset\n","optimized_performance_metrics = {}\n","\n","for key in ['train_data_w', 'train_data_m', 'train_data_q']:\n","    train_key = key\n","    test_key = key.replace('train', 'test')\n","\n","    X_train = vectorized_dataframes[train_key]\n","    y_train = dataframes[train_key]['Target'].values\n","    X_test = vectorized_dataframes[test_key]\n","    y_test = dataframes[test_key]['Target'].values\n","\n","    # Create and compile the optimized LSTM model\n","    lstm_model = create_optimized_lstm_model(\n","        (X_train.shape[1], X_train.shape[2]),\n","        lstm_units=64,\n","        dropout_rate=0.25,\n","        dense_units=32,\n","        learning_rate=0.0005\n","    )\n","\n","    # If you want to keep EarlyStopping but optimize for accuracy\n","    early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n","\n","    # Train the model with the optimized parameters, remove callbacks=[early_stopping] to train for full epochs\n","    lstm_model.fit(X_train, y_train, epochs=20, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\n","\n","    # Evaluate the model\n","    accuracy, precision, recall, f1 = evaluate_model(lstm_model, X_test, y_test)\n","\n","    # Store the metrics\n","    optimized_performance_metrics[test_key] = {\n","        'Accuracy': accuracy,\n","        'Precision': precision,\n","        'Recall': recall,\n","        'F1 Score': f1\n","    }\n","\n","# Print the optimized performance metrics for each dataset\n","for key, metrics in optimized_performance_metrics.items():\n","    print(f\"Optimized performance for {key}:\")\n","    for metric, value in metrics.items():\n","        print(f\"{metric}: {value:.4f}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yjHuiYPJbsqz","executionInfo":{"status":"ok","timestamp":1715461594839,"user_tz":240,"elapsed":49211,"user":{"displayName":"Rafiz Sadique","userId":"04609178330490164874"}},"outputId":"abeca37b-c8c4-498c-dc44-158b0e0b94a2"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","94/94 [==============================] - 5s 19ms/step - loss: 0.6879 - accuracy: 0.5630 - val_loss: 0.6866 - val_accuracy: 0.5560\n","Epoch 2/20\n","94/94 [==============================] - 1s 13ms/step - loss: 0.6851 - accuracy: 0.5643 - val_loss: 0.6865 - val_accuracy: 0.5560\n","Epoch 3/20\n","94/94 [==============================] - 2s 16ms/step - loss: 0.6843 - accuracy: 0.5655 - val_loss: 0.6862 - val_accuracy: 0.5573\n","Epoch 4/20\n","94/94 [==============================] - 1s 15ms/step - loss: 0.6832 - accuracy: 0.5685 - val_loss: 0.6858 - val_accuracy: 0.5580\n","Epoch 5/20\n","94/94 [==============================] - 1s 11ms/step - loss: 0.6823 - accuracy: 0.5710 - val_loss: 0.6857 - val_accuracy: 0.5540\n","Epoch 6/20\n","94/94 [==============================] - 1s 11ms/step - loss: 0.6812 - accuracy: 0.5708 - val_loss: 0.6864 - val_accuracy: 0.5553\n","Epoch 7/20\n","94/94 [==============================] - 1s 10ms/step - loss: 0.6801 - accuracy: 0.5730 - val_loss: 0.6868 - val_accuracy: 0.5573\n","235/235 [==============================] - 2s 4ms/step\n","Epoch 1/20\n","94/94 [==============================] - 6s 26ms/step - loss: 0.6910 - accuracy: 0.5368 - val_loss: 0.6929 - val_accuracy: 0.5240\n","Epoch 2/20\n","94/94 [==============================] - 1s 14ms/step - loss: 0.6900 - accuracy: 0.5372 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 3/20\n","94/94 [==============================] - 1s 12ms/step - loss: 0.6895 - accuracy: 0.5372 - val_loss: 0.6921 - val_accuracy: 0.5240\n","Epoch 4/20\n","94/94 [==============================] - 1s 10ms/step - loss: 0.6885 - accuracy: 0.5372 - val_loss: 0.6922 - val_accuracy: 0.5240\n","235/235 [==============================] - 2s 4ms/step\n","Epoch 1/20\n","94/94 [==============================] - 8s 23ms/step - loss: 0.6751 - accuracy: 0.5985 - val_loss: 0.6799 - val_accuracy: 0.5780\n","Epoch 2/20\n","94/94 [==============================] - 1s 11ms/step - loss: 0.6681 - accuracy: 0.5982 - val_loss: 0.6784 - val_accuracy: 0.5713\n","Epoch 3/20\n","94/94 [==============================] - 1s 10ms/step - loss: 0.6645 - accuracy: 0.5980 - val_loss: 0.6809 - val_accuracy: 0.5700\n","Epoch 4/20\n","94/94 [==============================] - 1s 11ms/step - loss: 0.6631 - accuracy: 0.6018 - val_loss: 0.6774 - val_accuracy: 0.5760\n","235/235 [==============================] - 2s 4ms/step\n","Optimized performance for test_data_w:\n","Accuracy: 0.4992\n","Precision: 0.4994\n","Recall: 0.9947\n","F1 Score: 0.6649\n","Optimized performance for test_data_m:\n","Accuracy: 0.6196\n","Precision: 0.6196\n","Recall: 1.0000\n","F1 Score: 0.7651\n","Optimized performance for test_data_q:\n","Accuracy: 0.2780\n","Precision: 0.7557\n","Recall: 0.0181\n","F1 Score: 0.0353\n"]}]},{"cell_type":"code","source":["# Count the number of positives and negatives in each training dataset\n","for key in ['train_data_w', 'train_data_m', 'train_data_q']:\n","    positives = dataframes[key]['Target'].sum()\n","    negatives = len(dataframes[key]) - positives\n","    print(f\"{key}: {positives} positives, {negatives} negatives\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AdVVZ_22fjuA","executionInfo":{"status":"ok","timestamp":1715461834108,"user_tz":240,"elapsed":181,"user":{"displayName":"Rafiz Sadique","userId":"04609178330490164874"}},"outputId":"fb4fb57a-826d-4f4c-c031-9624c412ccc7"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["train_data_w: 4222 positives, 3278 negatives\n","train_data_m: 4008 positives, 3492 negatives\n","train_data_q: 3044 positives, 4456 negatives\n"]}]},{"cell_type":"code","source":["from imblearn.over_sampling import RandomOverSampler\n","\n","# Initialize the random oversampler\n","ros = RandomOverSampler(random_state=42)\n","\n","# Balance the datasets by oversampling\n","balanced_dataframes = {}\n","for key in ['train_data_w', 'train_data_m', 'train_data_q']:\n","    X = dataframes[key][['cleaned_body']]  # Features (in this case, just the text)\n","    y = dataframes[key]['Target']  # Labels\n","    X_res, y_res = ros.fit_resample(X, y)\n","    balanced_dataframes[key] = X_res\n","    balanced_dataframes[key]['Target'] = y_res  # Add the resampled targets back\n","\n","    # Print the new balance\n","    positives = y_res.sum()\n","    negatives = len(y_res) - positives\n","    print(f\"Balanced {key}: {positives} positives, {negatives} negatives\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HjRvqbeQgXCR","executionInfo":{"status":"ok","timestamp":1715462045041,"user_tz":240,"elapsed":765,"user":{"displayName":"Rafiz Sadique","userId":"04609178330490164874"}},"outputId":"2fbefcc4-3da8-4152-b5a8-93c1f894a4df"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Balanced train_data_w: 4222 positives, 4222 negatives\n","Balanced train_data_m: 4008 positives, 4008 negatives\n","Balanced train_data_q: 4456 positives, 4456 negatives\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","# Function to create an optimized LSTM model\n","def create_optimized_lstm_model(input_shape, lstm_units=50, dropout_rate=0.3, dense_units=25, learning_rate=0.001):\n","    model = Sequential()\n","\n","    # LSTM layer with variable units\n","    model.add(LSTM(lstm_units, return_sequences=True, input_shape=input_shape))\n","    model.add(Dropout(dropout_rate))\n","\n","    # Another LSTM layer without return_sequences\n","    model.add(LSTM(lstm_units))\n","    model.add(Dropout(dropout_rate))\n","\n","    # Dense layer with variable units\n","    model.add(Dense(dense_units, activation='relu'))\n","    model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n","\n","    # Compile the model with a specified learning rate\n","    optimizer = Adam(learning_rate=learning_rate)\n","    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n","\n","    return model\n","\n","# Function to evaluate the model\n","def evaluate_model(model, X_test, y_test):\n","    # Predict the labels for the test set\n","    y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n","\n","    # Calculate evaluation metrics\n","    accuracy = accuracy_score(y_test, y_pred)\n","    precision = precision_score(y_test, y_pred)\n","    recall = recall_score(y_test, y_pred)\n","    f1 = f1_score(y_test, y_pred)\n","\n","    return accuracy, precision, recall, f1\n","\n","# Dictionary to store performance metrics\n","balanced_performance_metrics = {}\n","\n","# Train and evaluate for each balanced dataset\n","for key in ['train_data_w', 'train_data_m', 'train_data_q']:\n","    train_key = key\n","    test_key = key.replace('train', 'test')\n","\n","    # Vectorize the balanced training data\n","    X_train = vectorize_tweets(balanced_dataframes[train_key]['cleaned_body'].apply(lambda x: x.split()).tolist(), word2vec_model, max_length)\n","    y_train = balanced_dataframes[train_key]['Target'].values\n","    X_test = vectorized_dataframes[test_key]\n","    y_test = dataframes[test_key]['Target'].values\n","\n","    # Create and compile the optimized LSTM model\n","    lstm_model = create_optimized_lstm_model(\n","        (X_train.shape[1], X_train.shape[2]),\n","        lstm_units=64,\n","        dropout_rate=0.25,\n","        dense_units=32,\n","        learning_rate=0.0005\n","    )\n","\n","    # Train the model with the optimized parameters\n","    lstm_model.fit(X_train, y_train, epochs=20, batch_size=64, validation_split=0.2)\n","\n","    # Evaluate the model\n","    accuracy, precision, recall, f1 = evaluate_model(lstm_model, X_test, y_test)\n","\n","    # Store the metrics\n","    balanced_performance_metrics[test_key] = {\n","        'Accuracy': accuracy,\n","        'Precision': precision,\n","        'Recall': recall,\n","        'F1 Score': f1\n","    }\n","\n","# Print the balanced performance metrics for each dataset\n","for key, metrics in balanced_performance_metrics.items():\n","    print(f\"Balanced performance for {key}:\")\n","    for metric, value in metrics.items():\n","        print(f\"{metric}: {value:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3gTiMzP_gZcQ","executionInfo":{"status":"ok","timestamp":1715462226532,"user_tz":240,"elapsed":112587,"user":{"displayName":"Rafiz Sadique","userId":"04609178330490164874"}},"outputId":"abd66be2-8c2a-4e32-9924-66685da8eea7"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","106/106 [==============================] - 6s 23ms/step - loss: 0.6877 - accuracy: 0.5581 - val_loss: 0.7437 - val_accuracy: 0.2493\n","Epoch 2/20\n","106/106 [==============================] - 2s 16ms/step - loss: 0.6861 - accuracy: 0.5625 - val_loss: 0.7434 - val_accuracy: 0.2510\n","Epoch 3/20\n","106/106 [==============================] - 1s 13ms/step - loss: 0.6849 - accuracy: 0.5634 - val_loss: 0.7366 - val_accuracy: 0.2664\n","Epoch 4/20\n","106/106 [==============================] - 1s 10ms/step - loss: 0.6843 - accuracy: 0.5668 - val_loss: 0.7698 - val_accuracy: 0.2605\n","Epoch 5/20\n","106/106 [==============================] - 1s 10ms/step - loss: 0.6832 - accuracy: 0.5665 - val_loss: 0.7700 - val_accuracy: 0.2593\n","Epoch 6/20\n","106/106 [==============================] - 1s 10ms/step - loss: 0.6824 - accuracy: 0.5689 - val_loss: 0.7798 - val_accuracy: 0.2587\n","Epoch 7/20\n","106/106 [==============================] - 1s 10ms/step - loss: 0.6806 - accuracy: 0.5701 - val_loss: 0.7605 - val_accuracy: 0.2753\n","Epoch 8/20\n","106/106 [==============================] - 1s 10ms/step - loss: 0.6813 - accuracy: 0.5705 - val_loss: 0.7562 - val_accuracy: 0.2842\n","Epoch 9/20\n","106/106 [==============================] - 1s 10ms/step - loss: 0.6779 - accuracy: 0.5750 - val_loss: 0.7278 - val_accuracy: 0.3162\n","Epoch 10/20\n","106/106 [==============================] - 1s 10ms/step - loss: 0.6759 - accuracy: 0.5759 - val_loss: 0.7382 - val_accuracy: 0.2960\n","Epoch 11/20\n","106/106 [==============================] - 1s 10ms/step - loss: 0.6739 - accuracy: 0.5793 - val_loss: 0.7316 - val_accuracy: 0.3025\n","Epoch 12/20\n","106/106 [==============================] - 1s 11ms/step - loss: 0.6699 - accuracy: 0.5842 - val_loss: 0.7540 - val_accuracy: 0.3085\n","Epoch 13/20\n","106/106 [==============================] - 2s 15ms/step - loss: 0.6647 - accuracy: 0.5840 - val_loss: 0.7879 - val_accuracy: 0.2937\n","Epoch 14/20\n","106/106 [==============================] - 2s 16ms/step - loss: 0.6592 - accuracy: 0.5919 - val_loss: 0.7814 - val_accuracy: 0.3108\n","Epoch 15/20\n","106/106 [==============================] - 1s 11ms/step - loss: 0.6539 - accuracy: 0.5967 - val_loss: 0.7123 - val_accuracy: 0.4127\n","Epoch 16/20\n","106/106 [==============================] - 1s 10ms/step - loss: 0.6469 - accuracy: 0.6033 - val_loss: 0.7902 - val_accuracy: 0.3369\n","Epoch 17/20\n","106/106 [==============================] - 1s 11ms/step - loss: 0.6417 - accuracy: 0.6077 - val_loss: 0.7911 - val_accuracy: 0.3570\n","Epoch 18/20\n","106/106 [==============================] - 1s 10ms/step - loss: 0.6383 - accuracy: 0.6090 - val_loss: 0.7438 - val_accuracy: 0.3967\n","Epoch 19/20\n","106/106 [==============================] - 1s 10ms/step - loss: 0.6296 - accuracy: 0.6181 - val_loss: 0.7359 - val_accuracy: 0.3647\n","Epoch 20/20\n","106/106 [==============================] - 1s 10ms/step - loss: 0.6179 - accuracy: 0.6290 - val_loss: 0.7539 - val_accuracy: 0.3730\n","235/235 [==============================] - 1s 4ms/step\n","Epoch 1/20\n","101/101 [==============================] - 7s 18ms/step - loss: 0.6904 - accuracy: 0.5362 - val_loss: 0.7149 - val_accuracy: 0.3541\n","Epoch 2/20\n","101/101 [==============================] - 1s 10ms/step - loss: 0.6890 - accuracy: 0.5362 - val_loss: 0.7127 - val_accuracy: 0.3554\n","Epoch 3/20\n","101/101 [==============================] - 1s 10ms/step - loss: 0.6893 - accuracy: 0.5360 - val_loss: 0.7168 - val_accuracy: 0.3541\n","Epoch 4/20\n","101/101 [==============================] - 1s 10ms/step - loss: 0.6873 - accuracy: 0.5377 - val_loss: 0.7155 - val_accuracy: 0.3709\n","Epoch 5/20\n","101/101 [==============================] - 1s 10ms/step - loss: 0.6868 - accuracy: 0.5388 - val_loss: 0.7266 - val_accuracy: 0.3716\n","Epoch 6/20\n","101/101 [==============================] - 1s 10ms/step - loss: 0.6864 - accuracy: 0.5418 - val_loss: 0.7064 - val_accuracy: 0.3678\n","Epoch 7/20\n","101/101 [==============================] - 1s 10ms/step - loss: 0.6850 - accuracy: 0.5404 - val_loss: 0.7140 - val_accuracy: 0.3753\n","Epoch 8/20\n","101/101 [==============================] - 1s 10ms/step - loss: 0.6822 - accuracy: 0.5476 - val_loss: 0.7054 - val_accuracy: 0.4165\n","Epoch 9/20\n","101/101 [==============================] - 1s 12ms/step - loss: 0.6809 - accuracy: 0.5452 - val_loss: 0.7187 - val_accuracy: 0.3884\n","Epoch 10/20\n","101/101 [==============================] - 1s 14ms/step - loss: 0.6754 - accuracy: 0.5483 - val_loss: 0.7149 - val_accuracy: 0.3859\n","Epoch 11/20\n","101/101 [==============================] - 1s 14ms/step - loss: 0.6704 - accuracy: 0.5664 - val_loss: 0.6919 - val_accuracy: 0.5193\n","Epoch 12/20\n","101/101 [==============================] - 1s 14ms/step - loss: 0.6642 - accuracy: 0.5735 - val_loss: 0.7172 - val_accuracy: 0.4233\n","Epoch 13/20\n","101/101 [==============================] - 1s 10ms/step - loss: 0.6536 - accuracy: 0.5920 - val_loss: 0.6732 - val_accuracy: 0.5948\n","Epoch 14/20\n","101/101 [==============================] - 1s 10ms/step - loss: 0.6451 - accuracy: 0.6001 - val_loss: 0.6749 - val_accuracy: 0.5636\n","Epoch 15/20\n","101/101 [==============================] - 1s 10ms/step - loss: 0.6402 - accuracy: 0.6070 - val_loss: 0.7177 - val_accuracy: 0.4850\n","Epoch 16/20\n","101/101 [==============================] - 1s 10ms/step - loss: 0.6256 - accuracy: 0.6259 - val_loss: 0.7494 - val_accuracy: 0.4701\n","Epoch 17/20\n","101/101 [==============================] - 1s 11ms/step - loss: 0.6135 - accuracy: 0.6408 - val_loss: 0.7462 - val_accuracy: 0.4938\n","Epoch 18/20\n","101/101 [==============================] - 1s 10ms/step - loss: 0.6044 - accuracy: 0.6413 - val_loss: 0.7193 - val_accuracy: 0.5511\n","Epoch 19/20\n","101/101 [==============================] - 1s 10ms/step - loss: 0.5884 - accuracy: 0.6541 - val_loss: 0.7498 - val_accuracy: 0.5449\n","Epoch 20/20\n","101/101 [==============================] - 1s 10ms/step - loss: 0.5757 - accuracy: 0.6675 - val_loss: 0.7967 - val_accuracy: 0.5175\n","235/235 [==============================] - 2s 4ms/step\n","Epoch 1/20\n","112/112 [==============================] - 7s 17ms/step - loss: 0.6739 - accuracy: 0.5931 - val_loss: 0.8318 - val_accuracy: 0.1189\n","Epoch 2/20\n","112/112 [==============================] - 1s 11ms/step - loss: 0.6694 - accuracy: 0.5971 - val_loss: 0.7590 - val_accuracy: 0.3158\n","Epoch 3/20\n","112/112 [==============================] - 1s 11ms/step - loss: 0.6675 - accuracy: 0.5971 - val_loss: 0.8188 - val_accuracy: 0.1469\n","Epoch 4/20\n","112/112 [==============================] - 1s 11ms/step - loss: 0.6640 - accuracy: 0.5976 - val_loss: 0.8454 - val_accuracy: 0.2081\n","Epoch 5/20\n","112/112 [==============================] - 1s 11ms/step - loss: 0.6611 - accuracy: 0.5981 - val_loss: 0.9104 - val_accuracy: 0.1598\n","Epoch 6/20\n","112/112 [==============================] - 2s 15ms/step - loss: 0.6589 - accuracy: 0.5974 - val_loss: 0.8966 - val_accuracy: 0.1733\n","Epoch 7/20\n","112/112 [==============================] - 2s 17ms/step - loss: 0.6579 - accuracy: 0.6015 - val_loss: 0.8676 - val_accuracy: 0.1845\n","Epoch 8/20\n","112/112 [==============================] - 2s 19ms/step - loss: 0.6537 - accuracy: 0.6068 - val_loss: 0.8362 - val_accuracy: 0.3758\n","Epoch 9/20\n","112/112 [==============================] - 2s 16ms/step - loss: 0.6499 - accuracy: 0.6103 - val_loss: 0.8362 - val_accuracy: 0.2653\n","Epoch 10/20\n","112/112 [==============================] - 1s 12ms/step - loss: 0.6472 - accuracy: 0.6114 - val_loss: 0.8274 - val_accuracy: 0.3780\n","Epoch 11/20\n","112/112 [==============================] - 1s 10ms/step - loss: 0.6473 - accuracy: 0.6128 - val_loss: 0.8481 - val_accuracy: 0.2557\n","Epoch 12/20\n","112/112 [==============================] - 1s 11ms/step - loss: 0.6401 - accuracy: 0.6214 - val_loss: 0.7723 - val_accuracy: 0.4453\n","Epoch 13/20\n","112/112 [==============================] - 1s 11ms/step - loss: 0.6377 - accuracy: 0.6236 - val_loss: 0.7531 - val_accuracy: 0.5390\n","Epoch 14/20\n","112/112 [==============================] - 1s 11ms/step - loss: 0.6311 - accuracy: 0.6272 - val_loss: 0.8436 - val_accuracy: 0.4274\n","Epoch 15/20\n","112/112 [==============================] - 1s 11ms/step - loss: 0.6266 - accuracy: 0.6375 - val_loss: 0.8707 - val_accuracy: 0.3617\n","Epoch 16/20\n","112/112 [==============================] - 1s 12ms/step - loss: 0.6196 - accuracy: 0.6413 - val_loss: 0.8312 - val_accuracy: 0.4807\n","Epoch 17/20\n","112/112 [==============================] - 2s 16ms/step - loss: 0.6122 - accuracy: 0.6496 - val_loss: 0.8045 - val_accuracy: 0.4739\n","Epoch 18/20\n","112/112 [==============================] - 2s 15ms/step - loss: 0.6075 - accuracy: 0.6518 - val_loss: 0.7524 - val_accuracy: 0.5530\n","Epoch 19/20\n","112/112 [==============================] - 1s 11ms/step - loss: 0.5987 - accuracy: 0.6671 - val_loss: 0.7368 - val_accuracy: 0.5076\n","Epoch 20/20\n","112/112 [==============================] - 1s 11ms/step - loss: 0.5883 - accuracy: 0.6727 - val_loss: 0.8104 - val_accuracy: 0.4565\n","235/235 [==============================] - 2s 4ms/step\n","Balanced performance for test_data_w:\n","Accuracy: 0.4976\n","Precision: 0.4983\n","Recall: 0.8220\n","F1 Score: 0.6205\n","Balanced performance for test_data_m:\n","Accuracy: 0.5428\n","Precision: 0.6153\n","Recall: 0.6992\n","F1 Score: 0.6546\n","Balanced performance for test_data_q:\n","Accuracy: 0.4181\n","Precision: 0.7323\n","Recall: 0.3214\n","F1 Score: 0.4468\n"]}]}]}