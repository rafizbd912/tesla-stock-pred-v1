{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPC05H3yKaLThrYGsVMKBl2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["##Setup and Loading Data"],"metadata":{"id":"v1WAU81yN46h"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"uV6X9B4cMLSf"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import zipfile\n","import os\n","from datetime import datetime\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uy7yb3dXNAtg","executionInfo":{"status":"ok","timestamp":1715375006833,"user_tz":240,"elapsed":26832,"user":{"displayName":"Rafiz Sadique","userId":"04609178330490164874"}},"outputId":"7142d501-29cb-436d-8a02-f1ecdc6fe1fa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["base_path = '/content/drive/My Drive/519_Project_V1/content/'\n"],"metadata":{"id":"z8dyyxoZNImK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the paths to the CSV files\n","file_paths = [\n","    base_path + \"Tweet.csv\",\n","    base_path + \"Company_Tweet.csv\",\n","    base_path + \"tesla_2018_to_2020.csv\"\n","]"],"metadata":{"id":"7rgVjXdhNOH7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Reading in the data\n","tweets1_df = pd.read_csv(file_paths[0], index_col=0)\n","tweets1_companies_df = pd.read_csv(file_paths[1], index_col=0)\n","\n","# Convert post_date from Unix time to datetime\n","tweets1_df['post_date'] = pd.to_datetime(tweets1_df['post_date'], unit='s')\n","\n","# Keep only the tweets that mention TSLA\n","indices_to_keep = tweets1_companies_df[tweets1_companies_df['ticker_symbol'] == \"TSLA\"].index\n","tweets1_df = tweets1_df.loc[indices_to_keep]\n","\n","# Drop duplicates in the 'body' column\n","tweets1_df = tweets1_df.drop_duplicates(subset=['body'])\n","\n","print(f\"Shape of the processed tweets data: {tweets1_df.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tOKuLyvdNqEC","executionInfo":{"status":"ok","timestamp":1715375032959,"user_tz":240,"elapsed":26128,"user":{"displayName":"Rafiz Sadique","userId":"04609178330490164874"}},"outputId":"f51215ac-766f-498d-eb55-c20a6003129a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of the processed tweets data: (1016828, 6)\n"]}]},{"cell_type":"markdown","source":["##Data Processing"],"metadata":{"id":"pcrHLxisPqp6"}},{"cell_type":"code","source":["import re\n","\n","# Function to clean tweets\n","def clean_tweets(tweet):\n","    tweet = re.sub(\"@[A-Za-z0-9]+\", \"\", tweet)  # Remove @mentions\n","    tweet = re.sub(r\"https?\\:\\/\\/[\\w\\.\\/]+\", \"\", tweet)  # Remove URLs\n","    tweet = re.sub(r\"[^A-Za-z0-9 ]+\", \"\", tweet)  # Remove all non-alphanumeric characters except spaces\n","    tweet = tweet.lower()  # Convert to lowercase to avoid duplicates from capitalization\n","    tweet = \" \".join(tweet.split())  # Remove redundant spaces\n","    return tweet\n","\n","# Apply the cleaning function to each tweet in the 'body' column\n","tweets1_df['cleaned_body'] = tweets1_df['body'].apply(clean_tweets)\n","\n","# Tokenize the tweets by splitting them into words\n","tweets1_df['tokenized_body'] = tweets1_df['cleaned_body'].apply(lambda tweet: tweet.split())\n","\n","# Show the first few rows to verify the results\n","print(tweets1_df[['cleaned_body', 'tokenized_body']].head())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5cMrWLvJQQ_h","executionInfo":{"status":"ok","timestamp":1715375047182,"user_tz":240,"elapsed":14243,"user":{"displayName":"Rafiz Sadique","userId":"04609178330490164874"}},"outputId":"8df1f443-c5d0-4e06-f5f8-4346462503b6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                                         cleaned_body  \\\n","tweet_id                                                                \n","550442977802207232  gm tsla volkswagen pushes 2014 record recall t...   \n","550443807834402816  swing trading up to 891 return in 14 days swin...   \n","550443808606126081  swing trading up to 891 return in 14 days swin...   \n","550443809700851716  swing trading up to 891 return in 14 days swin...   \n","550443857142611968  swing trading up to 891 return in 14 days swin...   \n","\n","                                                       tokenized_body  \n","tweet_id                                                               \n","550442977802207232  [gm, tsla, volkswagen, pushes, 2014, record, r...  \n","550443807834402816  [swing, trading, up, to, 891, return, in, 14, ...  \n","550443808606126081  [swing, trading, up, to, 891, return, in, 14, ...  \n","550443809700851716  [swing, trading, up, to, 891, return, in, 14, ...  \n","550443857142611968  [swing, trading, up, to, 891, return, in, 14, ...  \n"]}]},{"cell_type":"code","source":["# Remove duplicates based on the 'cleaned_body' column\n","tweets1_df = tweets1_df.drop_duplicates(subset=['cleaned_body'])\n","\n","# Show the updated shape and some rows to confirm the changes\n","print(f\"Shape of the data after removing duplicates: {tweets1_df.shape}\")\n","print(tweets1_df[['cleaned_body', 'tokenized_body']].head())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a3D03wDIRSu7","executionInfo":{"status":"ok","timestamp":1715375047691,"user_tz":240,"elapsed":522,"user":{"displayName":"Rafiz Sadique","userId":"04609178330490164874"}},"outputId":"37bb4b7e-6a60-4f81-dd8c-cd06c9719e4a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of the data after removing duplicates: (979356, 8)\n","                                                         cleaned_body  \\\n","tweet_id                                                                \n","550442977802207232  gm tsla volkswagen pushes 2014 record recall t...   \n","550443807834402816  swing trading up to 891 return in 14 days swin...   \n","550447850857828352  top 10 searched stocks of 2014 aapl fb baba ts...   \n","550455534324047873  sneaky techniques for an options trading edge ...   \n","550459042787651584  yr tsla 4785 fb 4277 twtr 4364 aapl 3772 gld 2...   \n","\n","                                                       tokenized_body  \n","tweet_id                                                               \n","550442977802207232  [gm, tsla, volkswagen, pushes, 2014, record, r...  \n","550443807834402816  [swing, trading, up, to, 891, return, in, 14, ...  \n","550447850857828352  [top, 10, searched, stocks, of, 2014, aapl, fb...  \n","550455534324047873  [sneaky, techniques, for, an, options, trading...  \n","550459042787651584  [yr, tsla, 4785, fb, 4277, twtr, 4364, aapl, 3...  \n"]}]},{"cell_type":"code","source":["# Import Word2Vec from gensim\n","from gensim.models import Word2Vec\n","\n","# Prepare the list of tokenized tweets for Word2Vec training\n","tokenized_tweets = tweets1_df['tokenized_body'].tolist()\n","\n","# Train a Word2Vec model\n","word2vec_model = Word2Vec(sentences=tokenized_tweets, vector_size=100, window=5, min_count=1, workers=4)\n","\n","# Save the model for later use\n","word2vec_model.save(\"/content/drive/My Drive/519_Project_V1/LSTM/word2vec_model.bin\")\n","\n","# Check the word vector for a sample word, here 'tsla'\n","print(\"Vector for the word 'tsla':\")\n","print(word2vec_model.wv['tsla'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3C_UaKpLRz6P","executionInfo":{"status":"ok","timestamp":1715375278844,"user_tz":240,"elapsed":231154,"user":{"displayName":"Rafiz Sadique","userId":"04609178330490164874"}},"outputId":"7e147f4b-f617-45d8-826f-9ebe9947f1ab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Vector for the word 'tsla':\n","[ 1.18086398e-01  1.60969877e+00 -1.82996500e+00  5.57858944e-01\n"," -8.03792775e-01  2.46547442e-02 -4.27759677e-01 -1.08084452e+00\n"," -1.55517137e+00 -1.10799313e+00 -1.36482775e+00  5.64668953e-01\n"," -1.28370368e+00  1.38308477e+00  1.59839153e+00  1.23152995e+00\n"," -4.13377136e-01 -1.51204658e+00  2.17774487e+00  8.29834938e-01\n"," -4.02294785e-01 -1.85144496e+00  3.63160521e-01  2.08962369e+00\n","  7.02248886e-02 -9.93774235e-01  1.18192332e-02  1.08244725e-01\n","  2.73394704e-01  3.37885094e+00  1.51906684e-01  4.94257450e-01\n","  7.71756411e-01 -1.83977103e+00  9.84283611e-02 -2.13223279e-01\n","  6.07830435e-02 -2.14122608e-01  2.84290403e-01  1.63002908e+00\n"," -1.27496445e+00  1.34606525e-01  1.79450083e+00 -1.74696669e-01\n","  8.58965933e-01  3.22519720e-01  6.80763543e-01  2.49488920e-01\n","  1.82969582e+00  1.22949898e+00  5.66971958e-01  4.74415839e-01\n","  1.36350882e+00 -8.27246606e-01 -1.89014733e-01  2.76565433e-01\n"," -1.91572142e+00  4.34863836e-01 -8.50992054e-02  2.00577593e+00\n","  3.82249415e-01  9.99976635e-01 -3.21463490e+00  6.18281841e-01\n","  7.65928328e-02  2.82234693e+00  8.57534528e-01  2.99006891e+00\n","  4.01507020e-01 -3.80616814e-01  1.35977674e+00 -1.43543696e+00\n"," -1.90193903e+00 -9.37371016e-01 -1.35128057e+00 -8.51862967e-01\n"," -1.29579127e+00 -7.64595330e-01  3.21412587e+00 -1.22830927e+00\n","  1.70790076e-01 -8.90432112e-03 -1.72749960e+00  1.59989190e+00\n"," -9.50455487e-01  1.27257550e+00 -5.74543536e-01 -8.09446210e-04\n","  4.14962284e-02 -2.37404466e+00 -1.43236697e+00  9.09885615e-02\n","  1.33874273e+00 -3.08260411e-01 -1.09469700e+00 -8.19935679e-01\n","  1.24819964e-01 -2.63203263e+00 -1.49087727e+00  9.74251211e-01]\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import gc\n","\n","# Function to vectorize a batch of tweets\n","def vectorize_tweets_batch(tweets, word2vec_model, max_length):\n","    vectorized_tweets = []\n","    for tweet in tweets:\n","        tweet_vector = [word2vec_model.wv[word] for word in tweet if word in word2vec_model.wv]\n","        vectorized_tweets.append(tweet_vector)\n","\n","    # Pad the sequences to have the same length\n","    vectorized_tweets = pad_sequences(vectorized_tweets, maxlen=max_length, dtype='float32', padding='post', truncating='post', value=0.0)\n","    return vectorized_tweets\n","\n","# Define the maximum length of a tweet vector sequence\n","max_length = 30  # Adjust based on your data\n","\n","# Initialize list to store all vectorized tweets\n","vectorized_tweets_full = []\n","\n","# Process tweets in batches to save memory\n","batch_size = 1000  # Adjust based on your available RAM\n","for i in range(0, len(tweets1_df['tokenized_body']), batch_size):\n","    batch = tweets1_df['tokenized_body'][i:i + batch_size]\n","    vectorized_tweets = vectorize_tweets_batch(batch, word2vec_model, max_length)\n","    vectorized_tweets_full.append(vectorized_tweets)\n","    gc.collect()  # Collect garbage to free up memory\n","\n","# Concatenate all batches\n","vectorized_tweets_full = np.concatenate(vectorized_tweets_full, axis=0)\n","\n","# Assuming you have a 'Target' column for labels in tweets1_df\n","y = tweets1_df['Target'].values\n","\n","# Split the data into training and testing sets\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(vectorized_tweets_full, y, test_size=0.2, random_state=42)\n","\n","print(f\"Shape of X_train: {X_train.shape}\")\n","print(f\"Shape of X_test: {X_test.shape}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZpIjYQB8TFsF","outputId":"a14eb1b6-ae36-4f94-b970-e9cf50b8a8e1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Exception ignored in: <function _xla_gc_callback at 0x7f967bc57c70>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/lib/__init__.py\", line 98, in _xla_gc_callback\n","    def _xla_gc_callback(*args):\n","KeyboardInterrupt: \n","Exception ignored in: <function _xla_gc_callback at 0x7f967bc57c70>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/lib/__init__.py\", line 98, in _xla_gc_callback\n","    def _xla_gc_callback(*args):\n","KeyboardInterrupt: \n"]}]}]}